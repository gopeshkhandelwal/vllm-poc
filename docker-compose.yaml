services:
  # =============================================================
  # vLLM Architect - Text Generation Service
  # Intel XPU vLLM for Arc Pro B60 GPUs
  # OpenAI-compatible API at http://localhost:8001/v1
  # =============================================================
  vllm-architect:
    image: amr-registry.caas.intel.com/intelcloud/xpu-vllm-gil:1.0
    container_name: vllm_architect
    privileged: true
    network_mode: host
    shm_size: 64g
    devices:
      - /dev/dri:/dev/dri
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN:-}
      - HF_HOME=/llm/models/.hf_cache
      # Intel XPU: Use all 4 GPUs for 120B model
      - ONEAPI_DEVICE_SELECTOR=level_zero:0,1,2,3
      # Intel XPU: Disable torch.compile() - Triton backend incompatible with SYCL
      - TORCH_COMPILE_DISABLE=1
      # Intel XPU: Required for multi-processing on XPU devices
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
    volumes:
      - ./models/huggingface:/llm/models
      - ./start-vllm-architect.sh:/llm/start-vllm-architect.sh
      - ./logs:/llm/logs
    entrypoint: [""]
    command: ["/bin/bash", "/llm/start-vllm-architect.sh"]
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8001/v1/models >/dev/null || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 30
      start_period: 600s
    restart: unless-stopped

  orchestrator:
    build:
      context: ./langgraph/orchestrator
      dockerfile: Dockerfile
    container_name: langgraph_orchestrator
    network_mode: host
    environment:
      MODEL_NAME: "${MODEL_NAME:-openai/gpt-oss-120b}"
      VLLM_URL: "http://localhost:8001"
      # Production settings to prevent model repetition/degeneration
      TEMPERATURE: "${TEMPERATURE:-0.1}"
      REPETITION_PENALTY: "${REPETITION_PENALTY:-1.3}"
      FREQUENCY_PENALTY: "${FREQUENCY_PENALTY:-0.5}"
      MAX_OUTPUT_TOKENS: "${MAX_OUTPUT_TOKENS:-4096}"
      VLLM_TIMEOUT: "${VLLM_TIMEOUT:-120.0}"
    restart: on-failure
    depends_on:
      vllm-architect:
        condition: service_healthy

  comfyui-flux2:
    image: intel/llm-scaler-omni:0.1.0-b5
    container_name: comfyui_flux2_service
    privileged: true
    network_mode: "host"
    shm_size: "64g"
    devices:
      - /dev/dri:/dev/dri
    environment:
      - PYTHONUNBUFFERED=1
      - COMFYUI_MODEL_DIR=/llm/ComfyUI/models
    volumes:
      - ./models/comfyui:/llm/ComfyUI/models
      - ./start_comfyui.sh:/llm/ComfyUI/start_comfyui.sh
    command: [ "/bin/bash", "/llm/ComfyUI/start_comfyui.sh" ]
    healthcheck:
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
