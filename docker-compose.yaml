services:
  # =============================================================
  # vLLM Architect - Text Generation Service
  # Intel XPU vLLM for Arc Pro B60 GPUs
  # OpenAI-compatible API at http://localhost:8001/v1
  # =============================================================
  vllm-architect:
    image: xpu-vllm-gil:latest
    container_name: vllm_architect
    privileged: true
    network_mode: host
    shm_size: 32g
    ipc: host
    devices:
      - /dev/dri:/dev/dri
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN:-}
      - HF_HOME=/llm/models/.hf_cache
      - SYCL_CACHE_PERSISTENT=0
      - TORCH_COMPILE_DISABLE=1
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
      - http_proxy=http://proxy-dmz.intel.com:912
      - https_proxy=http://proxy-dmz.intel.com:912
      - HTTP_PROXY=http://proxy-dmz.intel.com:912
      - HTTPS_PROXY=http://proxy-dmz.intel.com:912
      - no_proxy=localhost,127.0.0.1
      - NO_PROXY=localhost,127.0.0.1
    volumes:
      - ./models/huggingface:/llm/models
      - ./start-vllm-architect.sh:/llm/start-vllm-architect.sh
    entrypoint: [""]
    command: ["/bin/bash", "/llm/start-vllm-architect.sh"]
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8001/v1/models >/dev/null || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 30
      start_period: 600s
    restart: unless-stopped

  # =============================================================
  # LangGraph Orchestrator
  # Routes requests between LLM and image generation services
  # =============================================================
  orchestrator:
    build:
      context: ./langgraph/orchestrator
      dockerfile: Dockerfile
    container_name: langgraph_orchestrator
    ports:
      - "9000:9000"
    environment:
      MODEL_NAME: "${MODEL_NAME}"
      VLLM_URL: "http://host.docker.internal:8001"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: on-failure
    networks:
      - lang_net

  # =============================================================
  # ComfyUI with Flux2 Image Generation
  # Intel-optimized image generation service
  # Web UI at http://localhost:3000
  # =============================================================
  comfyui-flux2:
    image: intel/llm-scaler-omni:0.1.0-b5
    container_name: comfyui_flux2_service
    privileged: true
    network_mode: host
    shm_size: 64g
    devices:
      - /dev/dri:/dev/dri
    environment:
      - PYTHONUNBUFFERED=1
      - http_proxy=http://proxy-dmz.intel.com:912
      - https_proxy=http://proxy-dmz.intel.com:912
      - no_proxy=localhost,127.0.0.1
      - COMFYUI_MODEL_DIR=/llm/ComfyUI/models
    volumes:
      - ./models/comfyui:/llm/ComfyUI/models
      - ./start_comfyui.sh:/llm/ComfyUI/start_comfyui.sh
    command: ["/bin/bash", "/llm/ComfyUI/start_comfyui.sh"]
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped

networks:
  lang_net:
    driver: bridge
