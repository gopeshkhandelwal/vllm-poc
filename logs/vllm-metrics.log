[W211 18:11:43.862540816 OperatorEntry.cpp:218] Warning: Warning only once for all operators,  other operators may also be overridden.
  Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: aten::geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -> Tensor(a!)
    registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6
  dispatch key: XPU
  previous kernel: registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:37
       new kernel: registered at /root/workspace/frameworks.ai.pytorch.ipex-gpu/build/Release/csrc/gpu/csrc/gpu/xpu/ATen/RegisterXPU_0.cpp:172 (function operator())
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:11:47 [utils.py:346] 
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:11:47 [utils.py:346]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:11:47 [utils.py:346]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.16.0rc1.dev35+g64a40a7ab
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:11:47 [utils.py:346]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /llm/models/openai/gpt-oss-20b
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:11:47 [utils.py:346]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:11:47 [utils.py:346] 
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:11:47 [utils.py:282] non-default args: {'model_tag': '/llm/models/openai/gpt-oss-20b', 'api_server_count': 1, 'host': '0.0.0.0', 'port': 8001, 'model': '/llm/models/openai/gpt-oss-20b', 'trust_remote_code': True, 'max_model_len': 4096, 'enforce_eager': True, 'served_model_name': ['openai/gpt-oss-20b'], 'enable_prefix_caching': True}
[0;36m(APIServer pid=8)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[0;36m(APIServer pid=8)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:11:57 [model.py:541] Resolved architecture: GptOssForCausalLM
[0;36m(APIServer pid=8)[0;0m ERROR 02-11 18:11:57 [repo_utils.py:47] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/llm/models/openai/gpt-oss-20b'. Use `repo_type` argument if needed., retrying 1 of 2
[0;36m(APIServer pid=8)[0;0m ERROR 02-11 18:11:59 [repo_utils.py:45] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/llm/models/openai/gpt-oss-20b'. Use `repo_type` argument if needed.
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:11:59 [model.py:1881] Downcasting torch.float32 to torch.bfloat16.
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:11:59 [model.py:1559] Using max model len 4096
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:11:59 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:11:59 [config.py:314] Overriding max cuda graph capture size to 1024 for performance.
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:11:59 [vllm.py:669] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=8)[0;0m WARNING 02-11 18:11:59 [_logger.py:68] Enforce eager set, overriding optimization level to -O0
[2026-02-11 18:12:03.831] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[W211 18:12:05.394497750 OperatorEntry.cpp:218] Warning: Warning only once for all operators,  other operators may also be overridden.
  Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: aten::geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -> Tensor(a!)
    registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6
  dispatch key: XPU
  previous kernel: registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:37
       new kernel: registered at /root/workspace/frameworks.ai.pytorch.ipex-gpu/build/Release/csrc/gpu/csrc/gpu/xpu/ATen/RegisterXPU_0.cpp:172 (function operator())
[0;36m(EngineCore_DP0 pid=443)[0;0m INFO 02-11 18:12:10 [core.py:96] Initializing a V1 LLM engine (v0.16.0rc1.dev35+g64a40a7ab) with config: model='/llm/models/openai/gpt-oss-20b', speculative_config=None, tokenizer='/llm/models/openai/gpt-oss-20b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=mxfp4, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=xpu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='openai_gptoss', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=openai/gpt-oss-20b, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False, 'fuse_act_padding': False}, 'max_cudagraph_capture_size': 1024, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': False}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=443)[0;0m /usr/local/lib/python3.12/dist-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
[0;36m(EngineCore_DP0 pid=443)[0;0m   _C._set_float32_matmul_precision(precision)
[0;36m(EngineCore_DP0 pid=443)[0;0m INFO 02-11 18:12:10 [parallel_state.py:1234] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.54.128.14:34755 backend=xccl
[0;36m(EngineCore_DP0 pid=443)[0;0m INFO 02-11 18:12:10 [parallel_state.py:1445] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=443)[0;0m INFO 02-11 18:12:10 [gpu_model_runner.py:4088] Starting to load model /llm/models/openai/gpt-oss-20b...
[0;36m(EngineCore_DP0 pid=443)[0;0m INFO 02-11 18:12:11 [xpu.py:50] Setting VLLM_KV_CACHE_LAYOUT to 'NHD' for XPU; only NHD layout is supported by XPU attention kernels.
[0;36m(EngineCore_DP0 pid=443)[0;0m INFO 02-11 18:12:11 [xpu.py:76] Using Flash Attention backend.
[0;36m(EngineCore_DP0 pid=443)[0;0m INFO 02-11 18:12:11 [mxfp4.py:163] Using ipex marlin backend on XPU
[0;36m(EngineCore_DP0 pid=443)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=443)[0;0m Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:02<00:05,  2.81s/it]
[0;36m(EngineCore_DP0 pid=443)[0;0m Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:04<00:02,  2.37s/it]
[0;36m(EngineCore_DP0 pid=443)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:08<00:00,  2.76s/it]
[0;36m(EngineCore_DP0 pid=443)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:08<00:00,  2.70s/it]
[0;36m(EngineCore_DP0 pid=443)[0;0m 
[0;36m(EngineCore_DP0 pid=443)[0;0m INFO 02-11 18:12:20 [default_loader.py:291] Loading weights took 8.31 seconds
[0;36m(EngineCore_DP0 pid=443)[0;0m INFO 02-11 18:12:21 [gpu_model_runner.py:4185] Model loading took 13.27 GiB memory and 9.705013 seconds
[0;36m(EngineCore_DP0 pid=443)[0;0m INFO 02-11 18:13:11 [gpu_worker.py:359] Available KV cache memory: 6.32 GiB
[0;36m(EngineCore_DP0 pid=443)[0;0m INFO 02-11 18:13:11 [kv_cache_utils.py:1307] GPU KV cache size: 137,920 tokens
[0;36m(EngineCore_DP0 pid=443)[0;0m INFO 02-11 18:13:11 [kv_cache_utils.py:1312] Maximum concurrency for 4,096 tokens per request: 43.55x
[0;36m(EngineCore_DP0 pid=443)[0;0m INFO 02-11 18:13:11 [utils.py:59] `_KV_CACHE_LAYOUT_OVERRIDE` variable detected. Setting KV cache layout to NHD.
[0;36m(EngineCore_DP0 pid=443)[0;0m INFO 02-11 18:13:11 [core.py:272] init engine (profile, create kv cache, warmup model) took 50.82 seconds
[0;36m(EngineCore_DP0 pid=443)[0;0m INFO 02-11 18:13:12 [vllm.py:669] Asynchronous scheduling is enabled.
[0;36m(EngineCore_DP0 pid=443)[0;0m WARNING 02-11 18:13:12 [_logger.py:68] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:13 [api_server.py:455] Supported tasks: ['generate']
[0;36m(APIServer pid=8)[0;0m WARNING 02-11 18:13:13 [_logger.py:68] For gpt-oss, we ignore --enable-auto-tool-choice and always enable tool use.
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [serving.py:184] Warming up chat template processing...
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [hf.py:316] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [serving.py:219] Chat template warmup completed in 654.4ms
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [api_server.py:460] Starting vLLM API server 0 on http://0.0.0.0:8001
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [launcher.py:47] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [launcher.py:47] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [launcher.py:47] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [launcher.py:47] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [launcher.py:47] Route: /load, Methods: GET
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [launcher.py:47] Route: /version, Methods: GET
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [launcher.py:47] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [launcher.py:47] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [launcher.py:47] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [launcher.py:47] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [launcher.py:47] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [launcher.py:47] Route: /pause, Methods: POST
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [launcher.py:47] Route: /resume, Methods: POST
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [launcher.py:47] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [launcher.py:47] Route: /metrics, Methods: GET
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [launcher.py:47] Route: /health, Methods: GET
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [launcher.py:47] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [launcher.py:47] Route: /ping, Methods: GET
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [launcher.py:47] Route: /ping, Methods: POST
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [launcher.py:47] Route: /invocations, Methods: POST
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [launcher.py:47] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [launcher.py:47] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [launcher.py:47] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [launcher.py:47] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [launcher.py:47] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [launcher.py:47] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [launcher.py:47] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:14 [launcher.py:47] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=8)[0;0m INFO:     Started server process [8]
[0;36m(APIServer pid=8)[0;0m INFO:     Waiting for application startup.
[0;36m(APIServer pid=8)[0;0m INFO:     Application startup complete.
[0;36m(APIServer pid=8)[0;0m INFO:     127.0.0.1:48406 - "GET /v1/models HTTP/1.1" 200 OK
[0;36m(APIServer pid=8)[0;0m INFO:     127.0.0.1:48410 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=8)[0;0m INFO:     127.0.0.1:48430 - "GET /v1/models HTTP/1.1" 200 OK
[0;36m(APIServer pid=8)[0;0m INFO:     127.0.0.1:48420 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=8)[0;0m INFO:     127.0.0.1:48432 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=8)[0;0m INFO:     127.0.0.1:48436 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=8)[0;0m INFO:     127.0.0.1:48442 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:25 [loggers.py:257] Engine 000: Avg prompt throughput: 36.8 tokens/s, Avg generation throughput: 15.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 57.7%
[0;36m(APIServer pid=8)[0;0m INFO:     127.0.0.1:48444 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=8)[0;0m INFO:     127.0.0.1:42380 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=8)[0;0m INFO:     127.0.0.1:42382 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:35 [loggers.py:257] Engine 000: Avg prompt throughput: 14.6 tokens/s, Avg generation throughput: 30.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 54.2%
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:45 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 54.2%
[0;36m(APIServer pid=8)[0;0m INFO:     127.0.0.1:36390 - "GET /v1/models HTTP/1.1" 200 OK
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:13:55 [loggers.py:257] Engine 000: Avg prompt throughput: 38.4 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 32.9%
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:14:05 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 52.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 32.9%
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:14:15 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 36.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 32.9%
[0;36m(APIServer pid=8)[0;0m INFO:     127.0.0.1:33738 - "GET /v1/models HTTP/1.1" 200 OK
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:14:25 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 49.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 32.9%
[0;36m(APIServer pid=8)[0;0m INFO:     127.0.0.1:36400 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:14:35 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 42.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 32.9%
[0;36m(APIServer pid=8)[0;0m INFO 02-11 18:14:45 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 32.9%
[0;36m(APIServer pid=8)[0;0m INFO:     127.0.0.1:42376 - "GET /v1/models HTTP/1.1" 200 OK
